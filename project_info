# Agent Consistency Research Project

## Research Question
How consistent are LLM-based agents in their behavior, and where does variance originate?

## Setup
- Agent: ReAct-style (reasoning + acting)
- Benchmark: HotpotQA (start with 200 examples)
- Models: GPT-4o, Llama 3.1 70B (via Together AI)
- Runs: 30-50 per task per model

## Metrics to Compute
1. Outcome consistency (% same final answer)
2. Action sequence similarity (edit distance between traces)
3. Reasoning trace similarity (semantic similarity of CoT)
4. First divergence point (which step do runs diverge?)
5. Failure mode distribution (categorize errors)

## Timeline
- Day 1-2: Setup + pilot experiments (25 runs)
- Day 3-5: Small-scale runs (500 runs), validate metrics
- Day 6-7: Full experiments (3000-9000 runs)
- Day 8-10: Analysis + key findings
- Day 11-14: Write paper, submit to arXiv

## File Structure
agent-consistency/
├── agent.py          # ReAct agent implementation
├── runner.py         # Experiment runner (async)
├── metrics.py        # Consistency metrics
├── analysis.py       # Results analysis
├── data/             # HotpotQA cache
└── results/          # Experiment logs


One Thing to Watch Out For
Cursor is great for code, but it might lose the research framing. Keep these questions visible somewhere:

What's surprising in the data?
What would change how people build agents?
What's the one-sentence finding for the abstract?